{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2479,
     "status": "ok",
     "timestamp": 1588905415215,
     "user": {
      "displayName": "Raspberry Pi",
      "photoUrl": "",
      "userId": "09371942930627630010"
     },
     "user_tz": 300
    },
    "id": "dQWooYS9S_2L",
    "outputId": "16f4ef40-0343-4dad-8286-67d6e8d3665a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd,re,numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "data_path = '/content/drive/My Drive/Colab Notebooks/tweets_data_analysis/training.1600000.processed.noemoticon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2RAAikAVLj6"
   },
   "outputs": [],
   "source": [
    "col_names = ['label','id','date','waste','username','text']\n",
    "data = pd.read_csv(data_path, encoding='latin-1',names=col_names)\n",
    "manual = pd.read_csv('/content/drive/My Drive/Colab Notebooks/tweets_data_analysis/testdata.manual.2009.06.14.csv', encoding='latin-1')\n",
    "\n",
    "def data_cleansing(string):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", string) \n",
    "    words = letters_only.lower().split()\n",
    "    output =  re.sub(r\"\\b[a-zA-Z]\\b\", \"\", \" \".join(words))                          \n",
    "    return (output)\n",
    "    \n",
    "data['text'] = data['text'].apply(lambda x:data_cleansing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58094,
     "status": "ok",
     "timestamp": 1588905470857,
     "user": {
      "displayName": "Raspberry Pi",
      "photoUrl": "",
      "userId": "09371942930627630010"
     },
     "user_tz": 300
    },
    "id": "Ytk2x40eV6kP",
    "outputId": "c8c8b20f-dfb7-4726-9044-ff722634d55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is ready\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing data\n",
    "x = data['text'].values\n",
    "y = data['label'].values\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# tfidf_Vect = TfidfVectorizer(stop_words='english',strip_accents='unicode', token_pattern=r'\\w{2,}', )#) #changed  tfidfvectorizer to use bigram \n",
    "# x = tfidf_Vect.fit_transform(x)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "print (\"tokenizer is ready\")\n",
    "# x = tokenizer.texts_to_matrix(x)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tc8P3FuEgSHM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tokenizer_json = tokenizer.word_index\n",
    "with open('/content/drive/My Drive/Colab Notebooks/tweets_data_analysis/token_wordindex.json', 'w') as f:\n",
    "    f.write(json.dumps(tokenizer_json))\n",
    "\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJxLryD2gRaL"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('/content/drive/My Drive/Colab Notebooks/tweets_data_analysis/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tKilUyDQi-kK"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = tokenizer.texts_to_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKOlfzKpfpdQ"
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print (len(y))\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdOtrrNFgFeA"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Dense(300,input_dim=1000, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc']) #optimizers adam, rmsprop, sgd\n",
    "\n",
    "history=model.fit(x_train,y_train, epochs=2, verbose=1, validation_data=(x_test,y_test), batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Auxdc3yDlh9K"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/Colab Notebooks/tweets_data_analysis/senti_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 628,
     "status": "error",
     "timestamp": 1588905471501,
     "user": {
      "displayName": "Raspberry Pi",
      "photoUrl": "",
      "userId": "09371942930627630010"
     },
     "user_tz": 300
    },
    "id": "L3uZjxzXgW9m",
    "outputId": "37977408-1a10-487a-d8c8-0b1ae2382795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token is ready\n",
      "(1, 1000)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b6797a20bce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "inp = \"this market is going is worst to eat me alive\"\n",
    "\n",
    "print (\"token is ready\")\n",
    "inp = tokenizer.texts_to_matrix([inp])\n",
    "print (inp.shape)\n",
    "print (inp)\n",
    "z = model.predict_classes(inp)\n",
    "print (z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8841,
     "status": "ok",
     "timestamp": 1588899900556,
     "user": {
      "displayName": "Raspberry Pi",
      "photoUrl": "",
      "userId": "09371942930627630010"
     },
     "user_tz": 300
    },
    "id": "62Uz8-Z3zf5g",
    "outputId": "847fe5ec-e73f-45a0-832d-f65f3a6d973b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.6/dist-packages (1.7.4)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.18.3)\n",
      "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-cpu==2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-hub==0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.7.0)\n",
      "Requirement already satisfied: PyInquirer==1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.0.3)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.28.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.34.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.9.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.4.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (3.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (2.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (3.2.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (1.0.8)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu==2.1.0->tensorflowjs) (2.1.1)\n",
      "Requirement already satisfied: regex>=2016.11.21 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2019.12.20)\n",
      "Requirement already satisfied: prompt-toolkit==1.0.14 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (1.0.14)\n",
      "Requirement already satisfied: Pygments>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-cpu==2.1.0->tensorflowjs) (46.1.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.2.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.1.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (2.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (4.0)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-cpu==2.1.0->tensorflowjs) (0.4.8)\n",
      "mkdir: cannot create directory ‘model’: File exists\n",
      "updating: model/ (stored 0%)\n",
      "updating: model/group1-shard1of1.bin (deflated 7%)\n",
      "updating: model/model.json (deflated 79%)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflowjs \n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "model = load_model('/content/drive/My Drive/Colab Notebooks/tweets_data_analysis/senti_model.tf')\n",
    "model.save('keras.h5')\n",
    "!mkdir model\n",
    "!tensorflowjs_converter --input_format keras keras.h5 model/\n",
    "!zip -r model.zip model "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMdEN3oPkye30ZYfxNM6gt0",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "mount_file_id": "1-7Guqt2_FD-V2CUxnXJxnH1zDsCv4pMo",
   "name": "tweet_sentimental_model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
